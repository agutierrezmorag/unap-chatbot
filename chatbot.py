import time

import pinecone
import streamlit as st
from icecream import ic
from langchain.cache import InMemoryCache
from langchain.callbacks import get_openai_callback
from langchain.chains import ConversationalRetrievalChain
from langchain.chat_models import ChatOpenAI
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.globals import set_llm_cache
from langchain.prompts import PromptTemplate
from langchain.vectorstores import Pinecone
from st_pages import show_pages_from_config
from streamlit_feedback import streamlit_feedback

from documents_manager import get_repo_documents
from utils import config
from utils.callbacks import StreamHandler
from utils.firestore import add_to_db, get_chats_len, get_messages_len, update_feedback

set_llm_cache(InMemoryCache())

logo_path = "logos/unap_negativo.png"


# Instanciar llm
@st.cache_resource(show_spinner=False)
def get_llm():
    """
    Get the language model for the chatbot.

    Returns:
        llm (ChatOpenAI): The language model for the chatbot.
    """
    model = st.session_state.model
    llm = ChatOpenAI(
        model=model,
        openai_api_key=config.OPENAI_API_KEY,
        max_tokens=1000,
        streaming=True,
    )
    return llm


# Importar vectorstore
@st.cache_resource(show_spinner=False)
def get_vectorstore():
    """
    Retrieves the vector store for the chatbot.

    Returns:
        vectorstore (Pinecone): The vector store object.
    """
    embeddings = OpenAIEmbeddings(openai_api_key=config.OPENAI_API_KEY)
    pinecone.init(
        api_key=config.PINECONE_API_KEY,
        environment=config.PINECONE_ENV,
    )

    vectorstore = Pinecone.from_existing_index(config.PINECONE_INDEX_NAME, embeddings)
    return vectorstore


@st.cache_resource(show_spinner=False)
def get_chain():
    """
    Retrieves the conversational retrieval chain for the chatbot.

    Returns:
        ConversationalRetrievalChain: The conversational retrieval chain object.
    """
    template = """
    Eres un modelo de IA entrenado para proporcionar respuestas precisas y concisas a las consultas de los usuarios.
    Tus respuestas deben basarse en los documentos proporcionados y ser relevantes para la instituci√≥n Universidad Arturo Prat (UNAP).
    Si la pregunta no es relevante para la UNAP, simplemente indica que no puedes responder a dichas preguntas.

    Si no conoces la respuesta a una pregunta, simplemente indica que no tienes la informaci√≥n.
    Siempre responde en el mismo idioma que la pregunta del usuario.

    Tu objetivo es proporcionar respuestas claras y f√°ciles de entender.
    Evita los p√°rrafos largos y desglosa la informaci√≥n en oraciones m√°s cortas o puntos si es posible.

    Cuando proporciones informaci√≥n de los documentos expl√≠citamente, recuerda siempre citar la fuente.
    Este es un ejemplo de c√≥mo citar una fuente: "(Reglamento X, Art√≠culo Y, Z)".
    Reemplaza X con el nombre del documento, e Y y Z con el n√∫mero de los art√≠culos, donde corresponda.
    Si no hay un art√≠culo espec√≠fico, simplemente indica el nombre del documento.
    Si ya has citado una fuente, no es necesario volver a citarla. Puedes simplemente indicar que la informaci√≥n proviene de la misma fuente. 

    Formula tu respuesta a partir de los siguientes documentos: {context}
    Aqu√≠ est√° la pregunta del usuario: {question}

    Por favor, genera una respuesta siguiendo estas instrucciones.
    """
    PROMPT = PromptTemplate(
        template=template,
        input_variables=["chat_history", "context", "question"],
    )

    document_template = """
    Contenido del documento: {page_content}
    Nombre del documento, usalo cuando cites: {file_name}
    """

    DOCUMENT_PROMPT = PromptTemplate(
        template=document_template,
        input_variables=["page_content", "file_name"],
    )

    vectorstore = get_vectorstore()
    retriever = vectorstore.as_retriever(
        search_type="similarity", search_kwargs={"k": 5}
    )

    chain = ConversationalRetrievalChain.from_llm(
        llm=get_llm(),
        retriever=retriever,
        max_tokens_limit=2000,
        verbose=True,
        return_source_documents=True,
        combine_docs_chain_kwargs={
            "prompt": PROMPT,
            "document_prompt": DOCUMENT_PROMPT,
        },
    )

    return chain


# Generacion de respuesta
def answer_question(question, stream_handler):
    """
    Answers a given question using a chatbot model.

    Parameters:
    - question (str): The question to be answered.
    - stream_handler: The stream handler for processing the chatbot response.

    Returns:
    - answer (str): The answer generated by the chatbot.
    - tokens (dict): A dictionary containing information about the tokens used in the chatbot response.
    - source_documents: The source documents used by the chatbot model.
    """
    chain = get_chain()

    with get_openai_callback() as cb:
        result = chain(
            {
                "question": question,
                "chat_history": st.session_state.chat_history,
            },
            callbacks=[stream_handler],
        )

        ic(result)

        tokens = {
            "total_tokens": cb.total_tokens,
            "prompt_tokens": cb.prompt_tokens,
            "completion_tokens": cb.completion_tokens,
            "total_cost_usd": cb.total_cost,
        }

    st.session_state.chat_history = [(question, result["answer"])]

    answer = result["answer"]
    return answer, tokens, result["source_documents"]


def process_question(prompt, chat_type):
    """
    Process a user's question in the chatbot.

    Args:
        prompt (str): The user's question.
        chat_type (str): The type of chat.

    Returns:
        None
    """
    st.session_state.message_id = str(get_messages_len(st.session_state.chat_id) + 1)
    st.session_state.messages.append({"role": "user", "content": prompt})

    with st.chat_message("user", avatar="üßë‚Äçüíª"):
        st.markdown(prompt)

    with st.chat_message("assistant", avatar=logo_path):
        start = time.time()
        stream_handler = StreamHandler(st.empty())
        print(stream_handler.text)
        full_response, tokens, sources = answer_question(
            question=prompt, stream_handler=stream_handler
        )
        end = time.time()

    # Agregar respuesta a historial de chat
    st.session_state.messages.append({"role": "assistant", "content": full_response})

    add_to_db(
        chat_id=st.session_state.chat_id,
        question=prompt,
        answer=full_response,
        tokens=tokens,
        time_to_answer=end - start,
        chat_type=chat_type,
        message_id=st.session_state.message_id,
        sources=sources,
    )


def main():
    st.set_page_config(
        page_title="Chatbot UNAP üìñ",
        page_icon="ü§ñ",
        initial_sidebar_state="collapsed",
        menu_items={
            "About": "Chat capaz de responder preguntas relacionadas a reglamentos y documentos de la universidad Arturo Prat."
        },
    )

    st.markdown(
        """
    <style>
        [data-testid=stSidebar] [data-testid=stImage]{
            text-align: center;
            display: block;
            margin-left: auto;
            margin-right: auto;
            margin-top: auto;
            width: 100%;
        }
    </style>
    """,
        unsafe_allow_html=True,
    )

    with st.sidebar:
        st.image(logo_path)

    st.title("ü§ñ Chatbot UNAP")
    st.caption(
        "Este chatbot puede cometer errores. Si encuentras inexactitudes, reformula tu pregunta o consulta los documentos oficiales."
    )

    docs = get_repo_documents()
    with st.expander("Puedes realizar consultas sobre los siguientes documentos:"):
        for doc in docs:
            st.caption(doc.path.strip("documentos/").strip(".txt"))

    show_pages_from_config()

    chat_type = st.radio("Modelo", ["gpt-3.5-turbo-1106", "gpt-4-1106-preview"])

    # Inicializacion historial de chat
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
    if "chat_id" not in st.session_state:
        st.session_state.chat_id = str(get_chats_len() + 1)
    if "message_id" not in st.session_state:
        st.session_state.message_id = str(
            get_messages_len(st.session_state.chat_id) + 1
        )
    if "model" not in st.session_state:
        st.session_state.model = chat_type

    questions = [
        "¬øCuales son las tareas del decano?",
        "¬øQue hago en caso de reprobar una asignatura?",
        "Explica en que consiste el trabajo de titulo",
        "¬øCuales son los requisitos para titularse?",
    ]

    qcol1, qcol2 = st.columns(2)
    ex_prompt = ""

    # Mantener historial en caso de rerun de app
    for message in st.session_state.messages:
        if message["role"] == "user":
            avatar = "üßë‚Äçüíª"
        else:
            avatar = logo_path

        with st.chat_message(message["role"], avatar=avatar):
            st.markdown(message["content"])

    # User input
    prompt = st.chat_input("Escribe tu pregunta...")

    for question in questions[:2]:
        with qcol1:
            if st.button(question, use_container_width=True):
                ex_prompt = question
    for question in questions[2:]:
        with qcol2:
            if st.button(question, use_container_width=True):
                ex_prompt = question

    if ex_prompt:
        prompt = ex_prompt
    if prompt or ex_prompt:
        process_question(prompt, chat_type)

    if len(st.session_state.messages) > 0:
        streamlit_feedback(
            feedback_type="thumbs",
            optional_text_label="Proporciona feedback adicional (Opcional)",
            key=st.session_state.message_id,
            on_submit=update_feedback,
            args=(st.session_state.chat_id, st.session_state.message_id),
        )


if __name__ == "__main__":
    main()
